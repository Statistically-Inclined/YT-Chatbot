{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebbdf315",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f88e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from pprint import pprint\n",
    "from langchain import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a187572e",
   "metadata": {},
   "source": [
    "### Setup API & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b09c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am doing well, thank you for asking! How are you today?\n",
      "{'input_tokens': 6, 'output_tokens': 16, 'total_tokens': 22, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Set up your API key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
    "\n",
    "# Set up Google Gemini LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.5, max_tokens=500)\n",
    "response = llm.invoke(\"Hi, How are you?\")\n",
    "print(response.content)\n",
    "print(response.usage_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e30423db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Extracted Video ID: z-moiQlcC6c\n"
     ]
    }
   ],
   "source": [
    "def extract_youtube_video_id(url: str) -> str:\n",
    "    pattern = r\"(?:v=|\\/)([0-9A-Za-z_-]{11})(?:[\\?&\\/]|$)\"\n",
    "    match = re.search(pattern, url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# Example usage\n",
    "yt_url = \"https://www.youtube.com/watch?v=z-moiQlcC6c&list=PLv8Cp2NvcY8AzNCATbDWMr8vqbJBYbxFW&index=8\"\n",
    "video_id = extract_youtube_video_id(yt_url)\n",
    "print(\"üéØ Extracted Video ID:\", video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d11d161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Available languages: ['en', 'en']\n",
      "‚úÖ Found transcript in preferred language: en\n",
      "\n",
      "üìÑ Transcript Preview:\n",
      " Hello Everyone, my name is Aarohi and welcome¬†\n",
      "to my channel. So guys in my today's video,¬†¬† I'll show you how to create your own CH GPT clone¬†\n",
      "using Langchain, streamlet and OpenAI API. First¬†¬† let me show you the demo of the app which we¬†\n",
      "are going to build and then we'll dive into¬†¬† the details of how to create that app. This is¬†\n",
      "my demo app and let's ask a question to this app¬†¬† which LLM are you using to generate responses?¬†\n",
      "So here you can see it is utilizing GPT-3. So¬†¬† GPT3 is a large la\n",
      "\n",
      "üìÑ Transcript Chunks Preview:\n",
      " [{'text': \"Hello Everyone, my name is Aarohi and welcome\\xa0\\nto my channel. So guys in my today's video,\\xa0\\xa0\", 'start': 0.84, 'duration': 4.08}, {'text': \"I'll show you how to create your own CH GPT clone\\xa0\\nusing Langchain, streamlet and OpenAI API. First\\xa0\\xa0\", 'start': 4.92, 'duration': 8.56}, {'text': \"let me show you the demo of the app which we\\xa0\\nare going to build and then we'll dive into\\xa0\\xa0\", 'start': 13.48, 'duration': 4.16}, {'text': \"the details of how to create that app. This is\\xa0\\nmy demo app and let's ask a question to this app\\xa0\\xa0\", 'start': 17.64, 'duration': 7.72}, {'text': 'which LLM are you using to generate responses?\\xa0\\nSo here you can see it is utilizing GPT-3. So\\xa0\\xa0', 'start': 25.36, 'duration': 9.32}, {'text': 'GPT3 is a large language model by OpenAI. The app\\xa0\\nwhich we are going to build today, In that app,\\xa0\\xa0', 'start': 34.68, 'duration': 7.16}, {'text': \"I have specified that I want to use GPT-3 LLM\\xa0\\nfor generating responses. Now let's ask the same\\xa0\\xa0\", 'start': 41.84, 'duration': 8.36}, {'text': \"question to the ChatGPT. okay let's open ChatGPT.\\xa0\\nSo let's ask the same question to the ChatGPT.\", 'start': 50.2, 'duration': 9.72}, {'text': 'So the chatGPT app is using GPT-4 to\\xa0\\ngenerate responses and this is again\\xa0\\xa0', 'start': 62.76, 'duration': 7.8}, {'text': 'another LLM by OpenAI and this LLM\\xa0\\nis more advanced than the GPT3. So,\\xa0\\xa0', 'start': 70.56, 'duration': 5.2}, {'text': \"why I'm using GPT3 because the cost to use GPT3\\xa0\\nis cheaper as compared to the GPT4 LLM. Now,\\xa0\\xa0\", 'start': 75.76, 'duration': 8.08}, {'text': \"let's ask another question to it- So\\xa0\\nlet's say I'm writing- Once Upon a time\\xa0\\xa0\", 'start': 83.84, 'duration': 6.08}, {'text': 'there lived and then hit enter see here you can\\xa0\\nsee that our app continues the story with the next\\xa0\\xa0', 'start': 91.44, 'duration': 8.48}, {'text': \"few lines. Okay! Now let's ask another question to\\xa0\\nour app - writer program of simple neural network.\", 'start': 99.92, 'duration': 20.28}, {'text': 'See here, we have a code for simple neural\\xa0\\nnetwork. So here you can see our app is\\xa0\\xa0', 'start': 120.2, 'duration': 5.36}, {'text': \"providing us answers. Now this kind of application\\xa0\\nwe are going to build today. Let's see how does\\xa0\\xa0\", 'start': 125.56, 'duration': 5.72}, {'text': 'it work? So, first we are typing a prompt we\\xa0\\nare asking a question and then we hit enter.\\xa0\\xa0', 'start': 131.28, 'duration': 6.24}, {'text': 'Then this triggers the call to the OpenAI API and\\xa0\\nthrough that API we access the GPT3 model which\\xa0\\xa0', 'start': 137.52, 'duration': 8.56}, {'text': 'is a large language model and then LLM processes\\xa0\\nthat input and generate a response and after that\\xa0\\xa0', 'start': 146.08, 'duration': 7.52}, {'text': 'we are displaying that response on a screen using\\xa0\\nstreamlit and what is streamlit? Streamlit is a\\xa0\\xa0', 'start': 153.6, 'duration': 7.08}, {'text': 'python library for developing web applications.\\xa0\\nIt is similar to flask or Django. Okay! This is\\xa0\\xa0', 'start': 160.68, 'duration': 5.4}, {'text': \"the overview what we are going to build today. Now\\xa0\\nlet's get started with the step by- step process.\\xa0\\xa0\", 'start': 166.08, 'duration': 5.56}, {'text': \"The first step is to set up the environment to\\xa0\\nrun chatGPT clone and for our today's tutorial,\\xa0\\xa0\", 'start': 171.64, 'duration': 6.52}, {'text': \"I'll be using Anaconda and if you don't already\\xa0\\nhave Anaconda installed then download and install\\xa0\\xa0\", 'start': 178.16, 'duration': 7.04}, {'text': 'it from the official Anaconda website and after\\xa0\\ninstalling Anaconda we will create a separate\\xa0\\xa0', 'start': 185.2, 'duration': 6.48}, {'text': \"environment for our today's task. Now open\\xa0\\nanaconda prompt, Okay! This is my anaconda\\xa0\\xa0\", 'start': 191.68, 'duration': 7.92}, {'text': \"prompt and now let's deactivate this base\\xa0\\nenvironment first. First step is to create an\\xa0\\xa0\", 'start': 199.6, 'duration': 7.6}, {'text': 'environment. So for that we are using conda create\\xa0\\nhyphen and then this is the environment name. So,\\xa0\\xa0', 'start': 207.2, 'duration': 7.6}, {'text': 'I want to create an environment with this name and\\xa0\\nI want to use Python 3.10 for it. So, hit enter.', 'start': 214.8, 'duration': 11.92}, {'text': 'Now the environment is created. Now we\\xa0\\nneed to activate the environment and\\xa0\\xa0', 'start': 231.84, 'duration': 3.6}, {'text': 'for that just copy this from here paste\\xa0\\nit here and hit enter our environment is\\xa0\\xa0', 'start': 235.44, 'duration': 8.48}, {'text': 'activated. Now inside this environment\\xa0\\nwe will install all the packages which\\xa0\\xa0', 'start': 243.92, 'duration': 5.48}, {'text': \"are needed for our today's class. Before\\xa0\\nthat, we will first upgrade the PIP and\\xa0\\xa0\", 'start': 249.4, 'duration': 5.64}, {'text': \"then we'll install the packages. So\\xa0\\nthis is the command to upgrade the\", 'start': 255.04, 'duration': 4.04}, {'text': \"Pip. Now, pip is upgraded. Now I have a\\xa0\\nrequirements.txt file in which I have I've\\xa0\\xa0\", 'start': 259.08, 'duration': 10.8}, {'text': \"mentioned all the modules which are needed for our\\xa0\\ntoday's class. So, let me go into the folder where\\xa0\\xa0\", 'start': 269.88, 'duration': 4.6}, {'text': 'I have that requirements. txt file. So this is the\\xa0\\nfolder where my code is. Let me open this folder,\\xa0\\xa0', 'start': 274.48, 'duration': 10.96}, {'text': \"here you can see this is my requirements.txt\\xa0\\nfile. Let's open this file. These are the\\xa0\\xa0\", 'start': 285.44, 'duration': 4.88}, {'text': 'requirements which we want to install and these\\xa0\\nrequirements are present in which folder inside\\xa0\\xa0', 'start': 290.32, 'duration': 4.36}, {'text': 'this second gen app folder, I have this\\xa0\\nchat GPT clone folder. Inside it, we have\\xa0\\xa0', 'start': 294.68, 'duration': 5.28}, {'text': 'requirements. so here just cd to the\\xa0\\nchatGPT clone folder and here we will\\xa0\\xa0', 'start': 299.96, 'duration': 8.12}, {'text': 'write pip install -r requirements.txt\\xa0\\nand this will install all the required', 'start': 308.08, 'duration': 8.84}, {'text': \"Packages. Now that our environment is set up,\\xa0\\nwe are ready to start creating the app. So let's\\xa0\\xa0\", 'start': 316.92, 'duration': 14.4}, {'text': \"understand the coding part now. So guys, for my\\xa0\\ntoday's tutorial the code which is required is\\xa0\\xa0\", 'start': 331.32, 'duration': 6.12}, {'text': 'here. the requirement. txt file is here .and\\xa0\\n.env file is here. okay, in this folder and I\\xa0\\xa0', 'start': 337.44, 'duration': 6.76}, {'text': \"have provided the link of this code in description\\xa0\\nsection so now I'm going to open this code. Here,\\xa0\\xa0\", 'start': 344.2, 'duration': 8.8}, {'text': 'first we are importing the required modules.\\xa0\\nThis is the OS module then we are importing the\\xa0\\xa0', 'start': 353.0, 'duration': 5.08}, {'text': 'streamlit module because we are creating a web\\xa0\\napplication using streamlit. Then we are using\\xa0\\xa0', 'start': 358.08, 'duration': 7.36}, {'text': 'this openAI module from langchain openai. Why\\xa0\\nwe need this module because today we are going\\xa0\\xa0', 'start': 365.44, 'duration': 6.84}, {'text': 'to build our chatGPT clone using the OpenAI API\\xa0\\nand to use that we need to import this openAi and\\xa0\\xa0', 'start': 372.28, 'duration': 9.4}, {'text': 'then these two lines are to load the environment\\xa0\\nvariables. Now, what are environment variables?\\xa0\\xa0', 'start': 381.68, 'duration': 6.64}, {'text': 'So guys, when you want to use OpenAI LLms then you\\xa0\\nneed an API key to access it and that API key we\\xa0\\xa0', 'start': 388.32, 'duration': 10.4}, {'text': 'will access that API key we will use in our code\\xa0\\nusing this module okay .where I have that API key,\\xa0\\xa0', 'start': 398.72, 'duration': 8.28}, {'text': \"where we need to write that API key, so for that\\xa0\\nwe have created one env file. So, when you'll open\\xa0\\xa0\", 'start': 407.0, 'duration': 7.32}, {'text': 'this .env file, inside this openAI API key and\\xa0\\nhere you have to mention your API key. This key\\xa0\\xa0', 'start': 414.32, 'duration': 9.52}, {'text': 'will not work. You have to create your own API\\xa0\\nkey. Now, let me show you how to create your own\\xa0\\xa0', 'start': 423.84, 'duration': 5.32}, {'text': 'API key. To generate your own open AI API key, you\\xa0\\nfirst need to signup on open.com and let me show\\xa0\\xa0', 'start': 429.16, 'duration': 11.32}, {'text': \"you once you'll sign up and log to your openAI\\xa0\\naccount and here you will see the API keys. So, in\\xa0\\xa0\", 'start': 440.48, 'duration': 9.28}, {'text': 'my case I have one API key. To create an API key\\xa0\\nyou need to click on this create new secret key\\xa0\\xa0', 'start': 449.76, 'duration': 7.52}, {'text': 'and here you can provide the name this is optional\\xa0\\nand then let it be like this only and then click\\xa0\\xa0', 'start': 457.28, 'duration': 6.68}, {'text': \"on create secret key. You'll get your API key.\\xa0\\nAfter that save that API key, copy this API key\\xa0\\xa0\", 'start': 463.96, 'duration': 7.44}, {'text': 'and then paste that API key over here and save\\xa0\\nthis .env file. Okay! Now, this is the environment\\xa0\\xa0', 'start': 471.4, 'duration': 8.16}, {'text': \"variable you needed for our today's class. then we\\xa0\\nhave this st.title. st.title sets the title of the\\xa0\\xa0\", 'start': 479.56, 'duration': 9.08}, {'text': 'streamlit web application. what is the title\\xa0\\n‚Äì ‚ÄúchatGPT like clone‚Äù and in our application\\xa0\\xa0', 'start': 488.64, 'duration': 7.16}, {'text': 'this is what we have here. We are retrieving\\xa0\\nour open AI API key from environment variables\\xa0\\xa0', 'start': 495.8, 'duration': 6.48}, {'text': 'and this line will only work if you have stored\\xa0\\nyour open a API key in the .env file. Over here,\\xa0\\xa0', 'start': 502.28, 'duration': 8.76}, {'text': 'we are storing the GPT 3.5 turbo as the default\\xa0\\nLLM model for our app. This means that whenever\\xa0\\xa0', 'start': 511.04, 'duration': 9.68}, {'text': 'user first visits your app this model will be\\xa0\\nused unless we change it manually and guys we\\xa0\\xa0', 'start': 520.72, 'duration': 6.76}, {'text': 'can also work with other LLMs of openAI like gp4\\xa0\\nand gpt4o in order to make this app more flexible.\\xa0\\xa0', 'start': 527.48, 'duration': 8.36}, {'text': 'What is the reason, why we are setting the default\\xa0\\nmodel to GPT 3.5- because every time when we ask\\xa0\\xa0', 'start': 535.84, 'duration': 6.4}, {'text': 'question in this app there is a cost associated\\xa0\\nwith it. OpenAI charges for the API usage based on\\xa0\\xa0', 'start': 542.24, 'duration': 7.08}, {'text': 'the number of tokens processed. Tokens are just\\xa0\\nindividual words in the input and the output.\\xa0\\xa0', 'start': 549.32, 'duration': 5.92}, {'text': \"Okay! So GPT 3.5 is cheaper as compared to GPT\\xa0\\n4 and 4o, that's why we have set our default llm\\xa0\\xa0\", 'start': 555.24, 'duration': 8.64}, {'text': 'model to GPT 3.5 over here. And then over here, in\\xa0\\nthis line we are initializing the chat history. So\\xa0\\xa0', 'start': 563.88, 'duration': 8.56}, {'text': 'here we are checking if messages are already\\xa0\\nthere in the streamlit session State and if\\xa0\\xa0', 'start': 572.44, 'duration': 7.88}, {'text': 'messages are not in streamlit session State then\\xa0\\nwe will create a list with the name of ‚Äúmessages‚Äù\\xa0\\xa0', 'start': 580.32, 'duration': 6.88}, {'text': 'and this list will be responsible to store all the\\xa0\\nprompts which user will type and all the replies\\xa0\\xa0', 'start': 587.2, 'duration': 9.0}, {'text': 'which our chatGPT will provide us. So all those\\xa0\\nmessages will be stored in this messages list\\xa0\\xa0', 'start': 596.2, 'duration': 7.2}, {'text': \"only. So let's understand this with the help of\\xa0\\nour app. Here right now we don't have any messages\\xa0\\xa0\", 'start': 603.4, 'duration': 6.12}, {'text': \"on the streamlit. so that means messages list will\\xa0\\nbe created. now let's ask a question here - any\", 'start': 609.52, 'duration': 10.16}, {'text': 'Question. Now we have a question and this\\xa0\\nis the reply. This is the prompt user add\\xa0\\xa0', 'start': 619.68, 'duration': 9.92}, {'text': 'and this is the response from the model. Now all\\xa0\\nthe messages from users or from bot, they will be\\xa0\\xa0', 'start': 629.6, 'duration': 8.44}, {'text': 'stored in this messages list. This is just storing\\xa0\\nthe chat history and chat history means all the\\xa0\\xa0', 'start': 638.04, 'duration': 7.84}, {'text': 'previous messages from the users and the chatGPT\\xa0\\nand then once we have all those messages in the\\xa0\\xa0', 'start': 645.88, 'duration': 6.76}, {'text': 'messages list then this code is responsible\\xa0\\nto display those messages on the screen. So,\\xa0\\xa0', 'start': 652.64, 'duration': 6.64}, {'text': 'we are able to see the messages over here on the\\xa0\\nstreamlit app. This is happening because of this\\xa0\\xa0', 'start': 659.28, 'duration': 6.32}, {'text': 'for Loop. Now this piece of code is responsible\\xa0\\nfor interaction between the user and the response\\xa0\\xa0', 'start': 665.6, 'duration': 7.44}, {'text': 'from the chat GPT. In this first line our app\\xa0\\nis accepting the messages from users through\\xa0\\xa0', 'start': 673.04, 'duration': 8.08}, {'text': 'this chat_input. Here you can see this chatinput,\\xa0\\nthis input box. So our app is accepting all the\\xa0\\xa0', 'start': 681.12, 'duration': 8.28}, {'text': 'messages from this input box and after\\xa0\\nthat we have this session state. Messages,\\xa0\\xa0', 'start': 689.4, 'duration': 8.08}, {'text': 'append messages, remember we have created\\xa0\\na messages list and in that list we are\\xa0\\xa0', 'start': 697.48, 'duration': 4.84}, {'text': 'going to store all the messages of the users\\xa0\\nand the chatbot. So whatever user will type,\\xa0\\xa0', 'start': 702.32, 'duration': 6.72}, {'text': 'when the user will hit enter that message will\\xa0\\ngo into the message list and we are appending\\xa0\\xa0', 'start': 709.04, 'duration': 6.8}, {'text': 'the message list and after that we are displaying\\xa0\\nthat message on the screen. This piece of code is\\xa0\\xa0', 'start': 715.84, 'duration': 7.68}, {'text': 'responsible to show to display the user message\\xa0\\nusing this streamlits chatore message. Then\\xa0\\xa0', 'start': 723.52, 'duration': 9.48}, {'text': 'with the help of this code, our chatGPT app will\\xa0\\nretrieve our response from the openAI model. The\\xa0\\xa0', 'start': 733.0, 'duration': 8.84}, {'text': 'open AI model which we have specified here, which\\xa0\\nopen AI model - this GPT 3.5 openai model. So,\\xa0\\xa0', 'start': 741.84, 'duration': 7.32}, {'text': 'here our app our chatGPT clone app will retrieves\\xa0\\nthe response from the open AI model and the chat\\xa0\\xa0', 'start': 750.28, 'duration': 10.36}, {'text': 'history will be the input to this model. Which\\xa0\\nchat history? what we have in the chat history?\\xa0\\xa0', 'start': 760.64, 'duration': 5.84}, {'text': 'We have the message which user typed( the prompt\\xa0\\nwhich user typed) that prompt is the input to this\\xa0\\xa0', 'start': 766.48, 'duration': 8.0}, {'text': 'openAI model. This line is responsible to write\\xa0\\nthe response in our app using std. WR stream and\\xa0\\xa0', 'start': 774.48, 'duration': 10.16}, {'text': 'then this response is also getting appending to\\xa0\\nthe messages list so remember messages list will\\xa0\\xa0', 'start': 784.64, 'duration': 7.04}, {'text': 'have all the prompts which user type and all the\\xa0\\nreplies which are chat GPT app shows. So here you\\xa0\\xa0', 'start': 791.68, 'duration': 7.88}, {'text': 'can see, so this is in the messages list this is\\xa0\\nalso in the messages list this is the prompt from\\xa0\\xa0', 'start': 799.56, 'duration': 6.28}, {'text': 'the user and this is the reply the response of the\\xa0\\nmodel and we are displaying that response on the\\xa0\\xa0', 'start': 805.84, 'duration': 7.56}, {'text': 'screen how first we are storing this response in\\xa0\\nthe messages list and then we are displaying that\\xa0\\xa0', 'start': 813.4, 'duration': 6.04}, {'text': 'messages list using this this line now to run this\\xa0\\napp you will just write streamlet run and then the\\xa0\\xa0', 'start': 819.44, 'duration': 11.16}, {'text': 'name of the Python file where we have our code and\\xa0\\nhit enter and this will open your app right .Now,\\xa0\\xa0', 'start': 830.6, 'duration': 7.84}, {'text': 'there is nothing on the screen that means the\\xa0\\nmessages list is empty. Now, we are going to type\\xa0\\xa0', 'start': 838.44, 'duration': 5.96}, {'text': \"a question- write a Python program. Let's say,\\xa0\\nwe just wrote this now I'll hit enter and this\\xa0\\xa0\", 'start': 844.4, 'duration': 11.48}, {'text': 'question will be appended in the messages list\\xa0\\nand the reply from our bot is also appended in\\xa0\\xa0', 'start': 855.88, 'duration': 8.48}, {'text': 'the messages list and we are displaying it. So\\xa0\\nguys, this is how you can create your own chat\\xa0\\xa0', 'start': 864.36, 'duration': 4.84}, {'text': 'GPT clone and the link of this code is given in\\xa0\\ndescription section. I hope this video is helpful\\xa0\\xa0', 'start': 869.2, 'duration': 6.84}, {'text': 'and guys if you like my content please like, share\\xa0\\nand subscribe my channel. thank you for watching!', 'start': 876.04, 'duration': 5.84}]\n"
     ]
    }
   ],
   "source": [
    "video_id = video_id\n",
    "priority_lang = \"en\"\n",
    "max_retries = 5\n",
    "\n",
    "\n",
    "def retry_get_transcript(video_id, lang_code, retries=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[lang_code])\n",
    "            return transcript\n",
    "        except Exception as e:\n",
    "            print(f\"üîÅ Retrying {lang_code} ({attempt + 1}/{retries}) due to error: {e}\")\n",
    "            time.sleep(2)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_transcript_with_priority(video_id, priority_lang=\"en\", retries=5):\n",
    "    try:\n",
    "        # Step 1: Get transcript list\n",
    "        transcripts = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "        all_langs = [t.language_code for t in transcripts]\n",
    "        print(f\"üéØ Available languages: {all_langs}\")\n",
    "\n",
    "        # Step 3: Try preferred language with retries\n",
    "        if priority_lang in all_langs:\n",
    "            transcript_chunks = retry_get_transcript(video_id, priority_lang, retries)\n",
    "            if transcript_chunks:\n",
    "                print(f\"‚úÖ Found transcript in preferred language: {priority_lang}\")\n",
    "                transcript = \" \".join(chunk[\"text\"] for chunk in transcript_chunks)\n",
    "\n",
    "            return transcript_chunks, transcript\n",
    "\n",
    "        # Step 4: Fallback to first available language with retries\n",
    "        if all_langs:\n",
    "            fallback_lang = all_langs[0]\n",
    "            transcript_chunks = retry_get_transcript(video_id, fallback_lang, retries)\n",
    "            if transcript_chunks:\n",
    "                print(f\"‚ö†Ô∏è Preferred language not found. Falling back to: {fallback_lang}\")\n",
    "                transcript = \" \".join(chunk[\"text\"] for chunk in transcript_chunks)\n",
    "\n",
    "            return transcript_chunks, transcript\n",
    "\n",
    "\n",
    "    except TranscriptsDisabled:\n",
    "        print(\"‚ùå Transcripts are disabled for this video.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó Unexpected error: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "# üîΩ Run the logic\n",
    "transcript_list, transcript = get_transcript_with_priority(video_id, retries=50)\n",
    "if transcript:\n",
    "    print(\"\\nüìÑ Transcript Preview:\\n\", transcript[:500])\n",
    "    print(\"\\nüìÑ Transcript Chunks Preview:\\n\", transcript_list[:500])\n",
    "else:\n",
    "    print(\"‚ùó Could not retrieve transcript after multiple attempts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7fecf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[Document(metadata={}, page_content=\"Hello Everyone, my name is Aarohi and welcome\\xa0\\nto my channel. So guys in my today's video,\\xa0\\xa0 I'll show you how to create your own CH GPT clone\\xa0\\nusing Langchain, streamlet and OpenAI API. First\\xa0\\xa0 let me show you the demo of the app which we\\xa0\\nare going to build and then we'll dive into\\xa0\\xa0 the details of how to create that app. This is\\xa0\\nmy demo app and let's ask a question to this app\\xa0\\xa0 which LLM are you using to generate responses?\\xa0\\nSo here you can see it is utilizing GPT-3. So\\xa0\\xa0 GPT3 is a large language model by OpenAI. The app\\xa0\\nwhich we are going to build today, In that app,\\xa0\\xa0 I have specified that I want to use GPT-3 LLM\\xa0\\nfor generating responses. Now let's ask the same\\xa0\\xa0 question to the ChatGPT. okay let's open ChatGPT.\\xa0\\nSo let's ask the same question to the ChatGPT. So the chatGPT app is using GPT-4 to\\xa0\\ngenerate responses and this is again\\xa0\\xa0 another LLM by OpenAI and this LLM\\xa0\\nis more advanced than the GPT3. So,\\xa0\\xa0 why I'm using GPT3 because the cost to use GPT3\\xa0\\nis cheaper as compared to the GPT4 LLM. Now,\\xa0\\xa0 let's ask another question to it- So\\xa0\\nlet's say I'm writing- Once Upon a time\\xa0\\xa0 there lived and then hit enter see here you can\\xa0\\nsee that our app continues the story with the next\\xa0\\xa0 few lines. Okay! Now let's ask another question to\\xa0\\nour app - writer program of simple neural network. See here, we have a code for simple neural\\xa0\\nnetwork. So here you can see our app is\\xa0\\xa0 providing us answers. Now this kind of application\\xa0\\nwe are going to build today. Let's see how does\\xa0\\xa0 it work? So, first we are typing a prompt we\\xa0\\nare asking a question and then we hit enter.\\xa0\\xa0 Then this triggers the call to the OpenAI API and\\xa0\\nthrough that API we access the GPT3 model which\\xa0\\xa0 is a large language model and then LLM processes\\xa0\\nthat input and generate a response and after that\\xa0\\xa0 we are displaying that response on a screen using\\xa0\\nstreamlit and what is streamlit? Streamlit is a\\xa0\\xa0 python library for developing web applications.\\xa0\\nIt is similar to flask or Django. Okay! This is\\xa0\\xa0 the overview what we are going to build today. Now\\xa0\\nlet's get started with the step by- step process.\\xa0\\xa0 The first step is to set up the environment to\\xa0\\nrun chatGPT clone and for our today's tutorial,\\xa0\\xa0 I'll be using Anaconda and if you don't already\\xa0\\nhave Anaconda installed then download and install\\xa0\\xa0 it from the official Anaconda website and after\\xa0\\ninstalling Anaconda we will create a separate\\xa0\\xa0 environment for our today's task. Now open\\xa0\\nanaconda prompt, Okay! This is my anaconda\\xa0\\xa0 prompt and now let's deactivate this base\\xa0\\nenvironment first. First step is to create an\\xa0\\xa0 environment. So for that we are using conda create\\xa0\\nhyphen and then this is the environment name. So,\\xa0\\xa0 I want to create an environment with this name and\\xa0\\nI want to use Python 3.10 for it. So, hit enter. Now the environment is created. Now we\\xa0\\nneed to activate the environment and\\xa0\\xa0 for that just copy this from here paste\\xa0\\nit here and hit enter our environment is\\xa0\\xa0 activated. Now inside this environment\\xa0\\nwe will install all the packages which\\xa0\\xa0 are needed for our today's class. Before\\xa0\\nthat, we will first upgrade the PIP and\\xa0\\xa0 then we'll install the packages. So\\xa0\\nthis is the command to upgrade the Pip. Now, pip is upgraded. Now I have a\\xa0\\nrequirements.txt file in which I have I've\\xa0\\xa0 mentioned all the modules which are needed for our\\xa0\\ntoday's class. So, let me go into the folder where\\xa0\\xa0 I have that requirements. txt file. So this is the\\xa0\\nfolder where my code is. Let me open this folder,\\xa0\\xa0 here you can see this is my requirements.txt\\xa0\\nfile. Let's open this file. These are the\\xa0\\xa0 requirements which we want to install and these\\xa0\\nrequirements are present in which folder inside\\xa0\\xa0 this second gen app folder, I have this\\xa0\\nchat GPT clone folder. Inside it, we have\\xa0\\xa0 requirements. so here just cd to the\\xa0\\nchatGPT clone folder and here we will\\xa0\\xa0 write pip install -r requirements.txt\\xa0\\nand this will install all the required Packages. Now that our environment is set up,\"),\n",
      " Document(metadata={}, page_content=\"file. Let's open this file. These are the\\xa0\\xa0 requirements which we want to install and these\\xa0\\nrequirements are present in which folder inside\\xa0\\xa0 this second gen app folder, I have this\\xa0\\nchat GPT clone folder. Inside it, we have\\xa0\\xa0 requirements. so here just cd to the\\xa0\\nchatGPT clone folder and here we will\\xa0\\xa0 write pip install -r requirements.txt\\xa0\\nand this will install all the required Packages. Now that our environment is set up,\\xa0\\nwe are ready to start creating the app. So let's\\xa0\\xa0 understand the coding part now. So guys, for my\\xa0\\ntoday's tutorial the code which is required is\\xa0\\xa0 here. the requirement. txt file is here .and\\xa0\\n.env file is here. okay, in this folder and I\\xa0\\xa0 have provided the link of this code in description\\xa0\\nsection so now I'm going to open this code. Here,\\xa0\\xa0 first we are importing the required modules.\\xa0\\nThis is the OS module then we are importing the\\xa0\\xa0 streamlit module because we are creating a web\\xa0\\napplication using streamlit. Then we are using\\xa0\\xa0 this openAI module from langchain openai. Why\\xa0\\nwe need this module because today we are going\\xa0\\xa0 to build our chatGPT clone using the OpenAI API\\xa0\\nand to use that we need to import this openAi and\\xa0\\xa0 then these two lines are to load the environment\\xa0\\nvariables. Now, what are environment variables?\\xa0\\xa0 So guys, when you want to use OpenAI LLms then you\\xa0\\nneed an API key to access it and that API key we\\xa0\\xa0 will access that API key we will use in our code\\xa0\\nusing this module okay .where I have that API key,\\xa0\\xa0 where we need to write that API key, so for that\\xa0\\nwe have created one env file. So, when you'll open\\xa0\\xa0 this .env file, inside this openAI API key and\\xa0\\nhere you have to mention your API key. This key\\xa0\\xa0 will not work. You have to create your own API\\xa0\\nkey. Now, let me show you how to create your own\\xa0\\xa0 API key. To generate your own open AI API key, you\\xa0\\nfirst need to signup on open.com and let me show\\xa0\\xa0 you once you'll sign up and log to your openAI\\xa0\\naccount and here you will see the API keys. So, in\\xa0\\xa0 my case I have one API key. To create an API key\\xa0\\nyou need to click on this create new secret key\\xa0\\xa0 and here you can provide the name this is optional\\xa0\\nand then let it be like this only and then click\\xa0\\xa0 on create secret key. You'll get your API key.\\xa0\\nAfter that save that API key, copy this API key\\xa0\\xa0 and then paste that API key over here and save\\xa0\\nthis .env file. Okay! Now, this is the environment\\xa0\\xa0 variable you needed for our today's class. then we\\xa0\\nhave this st.title. st.title sets the title of the\\xa0\\xa0 streamlit web application. what is the title\\xa0\\n‚Äì ‚ÄúchatGPT like clone‚Äù and in our application\\xa0\\xa0 this is what we have here. We are retrieving\\xa0\\nour open AI API key from environment variables\\xa0\\xa0 and this line will only work if you have stored\\xa0\\nyour open a API key in the .env file. Over here,\\xa0\\xa0 we are storing the GPT 3.5 turbo as the default\\xa0\\nLLM model for our app. This means that whenever\\xa0\\xa0 user first visits your app this model will be\\xa0\\nused unless we change it manually and guys we\\xa0\\xa0 can also work with other LLMs of openAI like gp4\\xa0\\nand gpt4o in order to make this app more flexible.\\xa0\\xa0 What is the reason, why we are setting the default\\xa0\\nmodel to GPT 3.5- because every time when we ask\\xa0\\xa0 question in this app there is a cost associated\\xa0\\nwith it. OpenAI charges for the API usage based on\\xa0\\xa0 the number of tokens processed. Tokens are just\\xa0\\nindividual words in the input and the output.\\xa0\\xa0 Okay! So GPT 3.5 is cheaper as compared to GPT\\xa0\\n4 and 4o, that's why we have set our default llm\\xa0\\xa0 model to GPT 3.5 over here. And then over here, in\\xa0\\nthis line we are initializing the chat history. So\\xa0\\xa0 here we are checking if messages are already\\xa0\\nthere in the streamlit session State and if\\xa0\\xa0 messages are not in streamlit session State then\\xa0\\nwe will create a list with the name of ‚Äúmessages‚Äù\\xa0\\xa0 and this list will be responsible to store all the\\xa0\\nprompts which user will type and all the replies\\xa0\\xa0 which our chatGPT will provide us. So all those\"),\n",
      " Document(metadata={}, page_content=\"this line we are initializing the chat history. So\\xa0\\xa0 here we are checking if messages are already\\xa0\\nthere in the streamlit session State and if\\xa0\\xa0 messages are not in streamlit session State then\\xa0\\nwe will create a list with the name of ‚Äúmessages‚Äù\\xa0\\xa0 and this list will be responsible to store all the\\xa0\\nprompts which user will type and all the replies\\xa0\\xa0 which our chatGPT will provide us. So all those\\xa0\\nmessages will be stored in this messages list\\xa0\\xa0 only. So let's understand this with the help of\\xa0\\nour app. Here right now we don't have any messages\\xa0\\xa0 on the streamlit. so that means messages list will\\xa0\\nbe created. now let's ask a question here - any Question. Now we have a question and this\\xa0\\nis the reply. This is the prompt user add\\xa0\\xa0 and this is the response from the model. Now all\\xa0\\nthe messages from users or from bot, they will be\\xa0\\xa0 stored in this messages list. This is just storing\\xa0\\nthe chat history and chat history means all the\\xa0\\xa0 previous messages from the users and the chatGPT\\xa0\\nand then once we have all those messages in the\\xa0\\xa0 messages list then this code is responsible\\xa0\\nto display those messages on the screen. So,\\xa0\\xa0 we are able to see the messages over here on the\\xa0\\nstreamlit app. This is happening because of this\\xa0\\xa0 for Loop. Now this piece of code is responsible\\xa0\\nfor interaction between the user and the response\\xa0\\xa0 from the chat GPT. In this first line our app\\xa0\\nis accepting the messages from users through\\xa0\\xa0 this chat_input. Here you can see this chatinput,\\xa0\\nthis input box. So our app is accepting all the\\xa0\\xa0 messages from this input box and after\\xa0\\nthat we have this session state. Messages,\\xa0\\xa0 append messages, remember we have created\\xa0\\na messages list and in that list we are\\xa0\\xa0 going to store all the messages of the users\\xa0\\nand the chatbot. So whatever user will type,\\xa0\\xa0 when the user will hit enter that message will\\xa0\\ngo into the message list and we are appending\\xa0\\xa0 the message list and after that we are displaying\\xa0\\nthat message on the screen. This piece of code is\\xa0\\xa0 responsible to show to display the user message\\xa0\\nusing this streamlits chatore message. Then\\xa0\\xa0 with the help of this code, our chatGPT app will\\xa0\\nretrieve our response from the openAI model. The\\xa0\\xa0 open AI model which we have specified here, which\\xa0\\nopen AI model - this GPT 3.5 openai model. So,\\xa0\\xa0 here our app our chatGPT clone app will retrieves\\xa0\\nthe response from the open AI model and the chat\\xa0\\xa0 history will be the input to this model. Which\\xa0\\nchat history? what we have in the chat history?\\xa0\\xa0 We have the message which user typed( the prompt\\xa0\\nwhich user typed) that prompt is the input to this\\xa0\\xa0 openAI model. This line is responsible to write\\xa0\\nthe response in our app using std. WR stream and\\xa0\\xa0 then this response is also getting appending to\\xa0\\nthe messages list so remember messages list will\\xa0\\xa0 have all the prompts which user type and all the\\xa0\\nreplies which are chat GPT app shows. So here you\\xa0\\xa0 can see, so this is in the messages list this is\\xa0\\nalso in the messages list this is the prompt from\\xa0\\xa0 the user and this is the reply the response of the\\xa0\\nmodel and we are displaying that response on the\\xa0\\xa0 screen how first we are storing this response in\\xa0\\nthe messages list and then we are displaying that\\xa0\\xa0 messages list using this this line now to run this\\xa0\\napp you will just write streamlet run and then the\\xa0\\xa0 name of the Python file where we have our code and\\xa0\\nhit enter and this will open your app right .Now,\\xa0\\xa0 there is nothing on the screen that means the\\xa0\\nmessages list is empty. Now, we are going to type\\xa0\\xa0 a question- write a Python program. Let's say,\\xa0\\nwe just wrote this now I'll hit enter and this\\xa0\\xa0 question will be appended in the messages list\\xa0\\nand the reply from our bot is also appended in\\xa0\\xa0 the messages list and we are displaying it. So\\xa0\\nguys, this is how you can create your own chat\\xa0\\xa0 GPT clone and the link of this code is given in\\xa0\\ndescription section. I hope this video is helpful\\xa0\\xa0 and guys if you like my content please like, share\"),\n",
      " Document(metadata={}, page_content=\"messages list is empty. Now, we are going to type\\xa0\\xa0 a question- write a Python program. Let's say,\\xa0\\nwe just wrote this now I'll hit enter and this\\xa0\\xa0 question will be appended in the messages list\\xa0\\nand the reply from our bot is also appended in\\xa0\\xa0 the messages list and we are displaying it. So\\xa0\\nguys, this is how you can create your own chat\\xa0\\xa0 GPT clone and the link of this code is given in\\xa0\\ndescription section. I hope this video is helpful\\xa0\\xa0 and guys if you like my content please like, share\\xa0\\nand subscribe my channel. thank you for watching!\")]\n"
     ]
    }
   ],
   "source": [
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=500)\n",
    "chunks = text_splitter.create_documents([transcript])\n",
    "\n",
    "# Check number of chunks\n",
    "pprint(len(chunks))\n",
    "pprint(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6920af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combine_prompt = '''You are an expert summarizer reviewing segment-level summaries of a YouTube video transcript. \n",
    "                          Your task is to create a **final, cohesive summary** that represents the overall content of the video.\n",
    "                          Please ensure the final summary is:\n",
    "                          - Concise and logically structured\n",
    "                          - Faithful to the main ideas discussed across the video\n",
    "                          - Written in clear and professional language\n",
    "\n",
    "                          Use bullet points to highlight key themes, and end with a brief concluding remark if appropriate.\n",
    "                          Below are the partial summaries:\n",
    "                          {text}\n",
    "                          Final Consolidated Summary:\n",
    "                       '''\n",
    "final_combine_prompt_template = PromptTemplate(input_variables=['text'], template=final_combine_prompt)\n",
    "\n",
    "\n",
    "chunks_prompt = '''You are analyzing a portion of a YouTube video transcript. Identify and summarize the **main ideas, insights, or recurring themes** in this chunk.\n",
    "                   Requirements:\n",
    "                   - Focus on important arguments, events, or opinions.\n",
    "                   - Avoid redundancy.\n",
    "                   - Use bullet points for clarity if needed.\n",
    "                    Transcript Chunk: {text}\n",
    "                    Summary:\n",
    "                '''\n",
    "map_prompt_template = PromptTemplate(input_variables=['text'], template=chunks_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bddd2381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This video tutorial demonstrates how to build a ChatGPT clone using '\n",
      " 'Langchain, Streamlit, and the OpenAI API, focusing on practical '\n",
      " 'implementation and cost-effectiveness. Key aspects include:\\n'\n",
      " '\\n'\n",
      " '*   **Project Setup:** Creating a dedicated Anaconda environment with Python '\n",
      " '3.10, upgrading pip, and installing necessary packages from a '\n",
      " '`requirements.txt` file.\\n'\n",
      " '*   **API Key Configuration:** Obtaining and securely storing an OpenAI API '\n",
      " \"key in a `.env` file for authentication and access to OpenAI's language \"\n",
      " 'models.\\n'\n",
      " '*   **Streamlit Application:** Building the user interface using Streamlit, '\n",
      " 'including setting the title and displaying the chat history.\\n'\n",
      " '*   **Language Model Integration:** Utilizing the `langchain` OpenAI module '\n",
      " 'to interact with language models, specifically GPT-3.5 Turbo for cost '\n",
      " 'efficiency.\\n'\n",
      " '*   **Chat History Management:** Implementing a \"messages\" list within '\n",
      " \"Streamlit's session state to store and display the conversation history, \"\n",
      " 'including user prompts and chatbot responses.\\n'\n",
      " \"*   **User Interaction:** Capturing user input through Streamlit's \"\n",
      " '`chat_input` component and displaying both user prompts and chatbot '\n",
      " 'responses in a conversational format.\\n'\n",
      " '*   **Execution:** Running the Streamlit application using the command '\n",
      " '`streamlit run <filename.py>`.\\n'\n",
      " '\\n'\n",
      " 'The tutorial provides a practical guide to creating a functional ChatGPT '\n",
      " 'clone while emphasizing cost considerations and efficient chat history '\n",
      " 'management. The complete code is available in the video description, and '\n",
      " 'viewers are encouraged to engage with the content.')\n"
     ]
    }
   ],
   "source": [
    "# Load summarize chain with map-reduce strategy\n",
    "summary_chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='map_reduce',\n",
    "    map_prompt=map_prompt_template,\n",
    "    combine_prompt=final_combine_prompt_template,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Run summarization on text chunks\n",
    "output = summary_chain.invoke(chunks)\n",
    "\n",
    "# Print the final summary\n",
    "pprint(output['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dacdbe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This video tutorial demonstrates how to build a ChatGPT clone using '\n",
      " 'Langchain, Streamlit, and the OpenAI API, focusing on practical '\n",
      " 'implementation and cost-effectiveness. Key aspects include:\\n'\n",
      " '\\n'\n",
      " '*   **Project Setup:** Creating a dedicated Anaconda environment with Python '\n",
      " '3.10, upgrading pip, and installing necessary packages from a '\n",
      " '`requirements.txt` file.\\n'\n",
      " '*   **API Key Configuration:** Obtaining and securely storing an OpenAI API '\n",
      " \"key in a `.env` file for authentication and access to OpenAI's language \"\n",
      " 'models.\\n'\n",
      " '*   **Streamlit Application:** Building the user interface using Streamlit, '\n",
      " 'including setting the title and displaying the chat history.\\n'\n",
      " '*   **Language Model Integration:** Utilizing the `langchain` OpenAI module '\n",
      " 'to interact with language models, specifically GPT-3.5 Turbo for cost '\n",
      " 'efficiency.\\n'\n",
      " '*   **Chat History Management:** Implementing a \"messages\" list within '\n",
      " \"Streamlit's session state to store and display the conversation history, \"\n",
      " 'including user prompts and chatbot responses.\\n'\n",
      " \"*   **User Interaction:** Capturing user input through Streamlit's \"\n",
      " '`chat_input` component and displaying both user prompts and chatbot '\n",
      " 'responses in a conversational format.\\n'\n",
      " '*   **Execution:** Running the Streamlit application using the command '\n",
      " '`streamlit run <filename.py>`.\\n'\n",
      " '\\n'\n",
      " 'The tutorial provides a practical guide to creating a functional ChatGPT '\n",
      " 'clone while emphasizing cost considerations and efficient chat history '\n",
      " 'management. The complete code is available in the video description, and '\n",
      " 'viewers are encouraged to engage with the content.')\n"
     ]
    }
   ],
   "source": [
    "pprint(output['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ff91a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
