{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33R54QYjCMAJ"
      },
      "source": [
        "### Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F7iZIBmB8_T",
        "outputId": "8d8fd24d-3f8b-45db-de6c-5ef654e65a22"
      },
      "outputs": [],
      "source": [
        "# !pip install -q youtube-transcript-api langchain-community langchain-openai faiss-cpu tiktoken python-dotenv google-api-python-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install youtube-transcript-api==1.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip show youtube-transcript-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hPcswe0tExci"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import yt_dlp\n",
        "import webvtt\n",
        "from pprint import pprint\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup API & Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am doing well, thank you for asking! How are you today?\n",
            "{'input_tokens': 6, 'output_tokens': 16, 'total_tokens': 22, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ],
      "source": [
        "# Set up your API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "\n",
        "# Set up Google Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.5, max_tokens=500)\n",
        "response = llm.invoke(\"Hi, How are you?\")\n",
        "print(response.content)\n",
        "print(response.usage_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZZrs-ijCTYt"
      },
      "source": [
        "### Extracting YT Video Transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Extracted Video ID: 67_aMPDk2zw\n"
          ]
        }
      ],
      "source": [
        "def extract_youtube_video_id(url: str) -> str:\n",
        "    parsed = urlparse(url)\n",
        "    \n",
        "    if parsed.hostname in (\"www.youtube.com\", \"youtube.com\"):\n",
        "        return parse_qs(parsed.query).get(\"v\", [None])[0]\n",
        "    \n",
        "    if parsed.hostname == \"youtu.be\":\n",
        "        return parsed.path.lstrip(\"/\")\n",
        "    \n",
        "    match = re.search(r\"(?:v=|\\/|embed\\/)([0-9A-Za-z_-]{11})\", url)\n",
        "    return match.group(1) if match else None\n",
        "    \n",
        "# Example usage\n",
        "yt_url = \"https://www.youtube.com/watch?v=67_aMPDk2zw&t=1s\"\n",
        "video_id = extract_youtube_video_id(yt_url)\n",
        "print(\"üéØ Extracted Video ID:\", video_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "\n",
        "# video_id = \"67_aMPDk2zw\" # only the ID, not full URL\n",
        "# try:\n",
        "#     # If you don‚Äôt care which language, this returns the ‚Äúbest‚Äù one\n",
        "#     transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=[\"en\"])\n",
        "\n",
        "#     # Flatten it to plain text\n",
        "#     transcript = \" \".join(chunk[\"text\"] for chunk in transcript_list)\n",
        "#     print(transcript)\n",
        "\n",
        "# except TranscriptsDisabled:\n",
        "#     print(\"No captions available for this video.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p9AXZycFIH6",
        "outputId": "74950ea5-66f9-4242-bf72-7b2d2ba3aaa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Available languages: ['en']\n",
            "‚úÖ Found transcript in preferred language: en\n",
            "\n",
            "üìÑ Transcript Preview:\n",
            " foreign [Music] has a curious parrot called buddy buddy has a great mimicking ability and a sharp memory buddy listens to all the conversations in Peter's home and can mimic them very accurately now when he hears feeling hungry I would like to have some for this case the probability of him saying Biryani cherries or food is much higher than the words such as bicycle or book but he doesn't understand the meaning of Biryani or food or cherries the way humans do all he is doing is using statistical\n",
            "\n",
            "üìÑ Transcript Chunks Preview:\n",
            " [{'text': 'foreign', 'start': 0.24, 'duration': 3.14}, {'text': '[Music]', 'start': 0.88, 'duration': 5.72}, {'text': 'has a curious parrot called buddy buddy', 'start': 3.38, 'duration': 5.139}, {'text': 'has a great mimicking ability and a', 'start': 6.6, 'duration': 3.3}, {'text': 'sharp memory', 'start': 8.519, 'duration': 3.361}, {'text': 'buddy listens to all the conversations', 'start': 9.9, 'duration': 4.5}, {'text': \"in Peter's home and can mimic them very\", 'start': 11.88, 'duration': 4.739}, {'text': 'accurately now when he hears feeling', 'start': 14.4, 'duration': 4.98}, {'text': 'hungry I would like to have some', 'start': 16.619, 'duration': 4.981}, {'text': 'for this case the probability of him', 'start': 19.38, 'duration': 4.319}, {'text': 'saying Biryani cherries or food is much', 'start': 21.6, 'duration': 4.679}, {'text': 'higher than the words such as bicycle or', 'start': 23.699, 'duration': 3.361}, {'text': 'book', 'start': 26.279, 'duration': 2.701}, {'text': \"but he doesn't understand the meaning of\", 'start': 27.06, 'duration': 4.139}, {'text': 'Biryani or food or cherries the way', 'start': 28.98, 'duration': 4.919}, {'text': 'humans do all he is doing is using', 'start': 31.199, 'duration': 5.161}, {'text': 'statistical probability along with some', 'start': 33.899, 'duration': 4.801}, {'text': 'Randomness to predict the next word or', 'start': 36.36, 'duration': 4.68}, {'text': 'set of words be only based on the past', 'start': 38.7, 'duration': 5.16}, {'text': 'conversations he has listened to we can', 'start': 41.04, 'duration': 5.1}, {'text': 'call Buddy a stochastic parrot', 'start': 43.86, 'duration': 4.56}, {'text': 'stochastic Means A system that is', 'start': 46.14, 'duration': 4.14}, {'text': 'characterized by Randomness or', 'start': 48.42, 'duration': 3.299}, {'text': 'probability', 'start': 50.28, 'duration': 3.48}, {'text': 'a language model is somewhat like a', 'start': 51.719, 'duration': 3.781}, {'text': 'stochastic parrot their computer', 'start': 53.76, 'duration': 3.779}, {'text': 'programs that use a technology called', 'start': 55.5, 'duration': 4.44}, {'text': 'neural networks to predict the next set', 'start': 57.539, 'duration': 4.981}, {'text': 'of words for a sentence for a simple', 'start': 59.94, 'duration': 4.38}, {'text': 'explanation of a neural network please', 'start': 62.52, 'duration': 3.779}, {'text': 'watch this particular video', 'start': 64.32, 'duration': 4.74}, {'text': \"just like how birdies strain on Peter's\", 'start': 66.299, 'duration': 5.341}, {'text': 'home conversations data set you can have', 'start': 69.06, 'duration': 4.919}, {'text': 'a language model that is trained on for', 'start': 71.64, 'duration': 5.159}, {'text': 'example all movie related articles from', 'start': 73.979, 'duration': 5.101}, {'text': 'Wikipedia and it will be able to predict', 'start': 76.799, 'duration': 3.841}, {'text': 'the next set of words for a movie', 'start': 79.08, 'duration': 4.5}, {'text': 'related sentence Gmail autocomplete is', 'start': 80.64, 'duration': 5.46}, {'text': 'one of the many applications that uses a', 'start': 83.58, 'duration': 4.32}, {'text': 'language model underneath', 'start': 86.1, 'duration': 3.839}, {'text': 'now that we have some understanding of a', 'start': 87.9, 'duration': 4.02}, {'text': \"language model let's understand what the\", 'start': 89.939, 'duration': 4.201}, {'text': 'heck is a large language model', 'start': 91.92, 'duration': 4.559}, {'text': \"let's go back to our buddy example our\", 'start': 94.14, 'duration': 5.7}, {'text': 'buddy got some Divine super power and', 'start': 96.479, 'duration': 6.361}, {'text': \"now he can listen to Peter's neighbors\", 'start': 99.84, 'duration': 5.16}, {'text': 'conversations conversations that are', 'start': 102.84, 'duration': 4.68}, {'text': 'happening in schools and universities in', 'start': 105.0, 'duration': 5.399}, {'text': 'the town in fact not only in his town', 'start': 107.52, 'duration': 6.18}, {'text': 'but all the towns across the world', 'start': 110.399, 'duration': 5.821}, {'text': 'with this extra power and knowledge now', 'start': 113.7, 'duration': 4.919}, {'text': 'buddy can complete the next set of words', 'start': 116.22, 'duration': 4.2}, {'text': 'on a history subject', 'start': 118.619, 'duration': 4.561}, {'text': 'give your nutrition advice or even write', 'start': 120.42, 'duration': 6.0}, {'text': 'a poem like our powerful parrot body', 'start': 123.18, 'duration': 5.999}, {'text': 'large language models are trained on a', 'start': 126.42, 'duration': 5.039}, {'text': 'huge volume of data such as Wikipedia', 'start': 129.179, 'duration': 4.861}, {'text': 'articles Google news articles online', 'start': 131.459, 'duration': 4.261}, {'text': 'books and so on', 'start': 134.04, 'duration': 4.62}, {'text': 'if you look inside the llm you will find', 'start': 135.72, 'duration': 5.04}, {'text': 'a neural network containing trillions of', 'start': 138.66, 'duration': 4.38}, {'text': 'parameters that can capture more complex', 'start': 140.76, 'duration': 5.1}, {'text': 'patterns and nuances in a language chat', 'start': 143.04, 'duration': 5.16}, {'text': 'GPT is an application that uses llm', 'start': 145.86, 'duration': 6.9}, {'text': 'called gpt3 or gpt4 behind the scenes', 'start': 148.2, 'duration': 7.38}, {'text': 'examples of llms are Palm 2 by Google', 'start': 152.76, 'duration': 5.76}, {'text': 'and llama by meta', 'start': 155.58, 'duration': 5.939}, {'text': 'on top of statistical predictions llm', 'start': 158.52, 'duration': 4.939}, {'text': 'uses another approach called', 'start': 161.519, 'duration': 4.141}, {'text': 'reinforcement learning with human', 'start': 163.459, 'duration': 6.041}, {'text': \"feedback rlhf let's understand this once\", 'start': 165.66, 'duration': 6.299}, {'text': 'again with Buddy one day Peter was', 'start': 169.5, 'duration': 4.319}, {'text': 'having a conversation with his cute', 'start': 171.959, 'duration': 4.701}, {'text': 'little two-year-old son', 'start': 173.819, 'duration': 7.881}, {'text': \"don't eat too much bananas else\", 'start': 176.66, 'duration': 5.04}, {'text': 'hearing this Peter realized that buddy', 'start': 183.3, 'duration': 4.439}, {'text': 'has been listening to the conversations', 'start': 185.879, 'duration': 5.761}, {'text': 'from abusive parents in his town what he', 'start': 187.739, 'duration': 6.061}, {'text': 'said was the effect of that', 'start': 191.64, 'duration': 4.319}, {'text': 'Peter then starts skipping a close eye', 'start': 193.8, 'duration': 4.5}, {'text': 'on what buddy is saying for a same', 'start': 195.959, 'duration': 4.2}, {'text': 'question buddy can produce multiple', 'start': 198.3, 'duration': 5.159}, {'text': 'answers and all Peter has to do is tell', 'start': 200.159, 'duration': 5.821}, {'text': 'him which one is toxic and which one is', 'start': 203.459, 'duration': 3.541}, {'text': 'not', 'start': 205.98, 'duration': 3.119}, {'text': \"after this training buddy doesn't use\", 'start': 207.0, 'duration': 4.019}, {'text': 'any toxic language', 'start': 209.099, 'duration': 4.681}, {'text': 'while training chat GPT open air used a', 'start': 211.019, 'duration': 5.22}, {'text': 'similar approach of human intervention', 'start': 213.78, 'duration': 4.44}, {'text': 'rlhf', 'start': 216.239, 'duration': 5.22}, {'text': 'open air used a huge Workforce of humans', 'start': 218.22, 'duration': 6.599}, {'text': 'to make chat GPT less toxic while llms', 'start': 221.459, 'duration': 5.161}, {'text': \"are very powerful they don't have any\", 'start': 224.819, 'duration': 4.381}, {'text': 'subjective experience emotions or', 'start': 226.62, 'duration': 5.179}, {'text': 'Consciousness that we as humans have', 'start': 229.2, 'duration': 5.28}, {'text': 'llms work purely based on the data that', 'start': 231.799, 'duration': 4.241}, {'text': 'they have been trained on I hope you', 'start': 234.48, 'duration': 3.66}, {'text': 'like this short explanation which was', 'start': 236.04, 'duration': 4.559}, {'text': 'based on analogy obviously the technical', 'start': 238.14, 'duration': 4.98}, {'text': 'working of this thing is little', 'start': 240.599, 'duration': 4.261}, {'text': 'different than analogy but this should', 'start': 243.12, 'duration': 4.08}, {'text': 'give you a good intuition on this topic', 'start': 244.86, 'duration': 4.26}, {'text': 'if you like this video please share with', 'start': 247.2, 'duration': 5.899}, {'text': 'those who are curious about this topic', 'start': 249.12, 'duration': 3.979}, {'text': 'foreign', 'start': 255.18, 'duration': 3.0}]\n"
          ]
        }
      ],
      "source": [
        "## Extracting Transcript\n",
        "\n",
        "video_id = video_id\n",
        "priority_lang = \"en\"\n",
        "max_retries = 5\n",
        "    \n",
        "\n",
        "def retry_get_transcript(video_id, lang_code, retries=5):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[lang_code])\n",
        "            return transcript\n",
        "        except Exception as e:\n",
        "            print(f\"üîÅ Retrying {lang_code} ({attempt + 1}/{retries}) due to error: {e}\")\n",
        "            time.sleep(2)\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_transcript_with_priority(video_id, priority_lang=\"en\", retries=5):\n",
        "    try:\n",
        "        # Step 1: Get transcript list\n",
        "        transcripts = YouTubeTranscriptApi.list_transcripts(video_id)\n",
        "        all_langs = [t.language_code for t in transcripts]\n",
        "        print(f\"üéØ Available languages: {all_langs}\")\n",
        "\n",
        "        # Step 3: Try preferred language with retries\n",
        "        if priority_lang in all_langs:\n",
        "            transcript_chunks = retry_get_transcript(video_id, priority_lang, retries)\n",
        "            if transcript_chunks:\n",
        "                print(f\"‚úÖ Found transcript in preferred language: {priority_lang}\")\n",
        "                transcript = \" \".join(chunk[\"text\"] for chunk in transcript_chunks)\n",
        "\n",
        "            return transcript_chunks, transcript\n",
        "\n",
        "        # Step 4: Fallback to first available language with retries\n",
        "        if all_langs:\n",
        "            fallback_lang = all_langs[0]\n",
        "            transcript_chunks = retry_get_transcript(video_id, fallback_lang, retries)\n",
        "            if transcript_chunks:\n",
        "                print(f\"‚ö†Ô∏è Preferred language not found. Falling back to: {fallback_lang}\")\n",
        "                transcript = \" \".join(chunk[\"text\"] for chunk in transcript_chunks)\n",
        "\n",
        "            return transcript_chunks, transcript\n",
        "\n",
        "\n",
        "    except TranscriptsDisabled:\n",
        "        print(\"‚ùå Transcripts are disabled for this video.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùó Unexpected error: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "# üîΩ Run the logic\n",
        "transcript_list, transcript = get_transcript_with_priority(video_id, retries=5)\n",
        "if transcript:\n",
        "    print(\"\\nüìÑ Transcript Preview:\\n\", transcript[:500])\n",
        "    print(\"\\nüìÑ Transcript Chunks Preview:\\n\", transcript_list[:500])\n",
        "else:\n",
        "    print(\"‚ùó Could not retrieve transcript after multiple attempts.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Transcript List: {transcript_list[:5]}\")\n",
        "print(f\"Transcript: {transcript[:100]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading subtitles...\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=zYGDpG-pTho&t=456s\n",
            "[youtube] zYGDpG-pTho: Downloading webpage\n",
            "[youtube] zYGDpG-pTho: Downloading tv client config\n",
            "[youtube] zYGDpG-pTho: Downloading tv player API JSON\n",
            "[youtube] zYGDpG-pTho: Downloading ios player API JSON\n",
            "[youtube] zYGDpG-pTho: Downloading player fc2a56a5-main\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: [youtube] Falling back to generic n function search\n",
            "         player = https://www.youtube.com/s/player/fc2a56a5/player_ias.vflset/en_US/base.js\n",
            "WARNING: [youtube] zYGDpG-pTho: nsig extraction failed: Some formats may be missing\n",
            "         n = jopnKWHFxUulVqO ; player = https://www.youtube.com/s/player/fc2a56a5/player_ias.vflset/en_US/base.js\n",
            "         Please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
            "WARNING: [youtube] zYGDpG-pTho: nsig extraction failed: Some formats may be missing\n",
            "         n = Id1dQbFg0AkRnKh ; player = https://www.youtube.com/s/player/fc2a56a5/player_ias.vflset/en_US/base.js\n",
            "         Please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
            "WARNING: [youtube] zYGDpG-pTho: nsig extraction failed: Some formats may be missing\n",
            "         n = _7ffU55wsHyyjuw ; player = https://www.youtube.com/s/player/fc2a56a5/player_ias.vflset/en_US/base.js\n",
            "         Please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
            "WARNING: [youtube] zYGDpG-pTho: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[youtube] zYGDpG-pTho: Downloading m3u8 information\n",
            "[info] zYGDpG-pTho: Downloading subtitles: en\n",
            "[info] zYGDpG-pTho: Downloading 1 format(s): 616+234\n",
            "Deleting existing file transcript_temp.en.vtt\n",
            "[info] Writing video subtitles to: transcript_temp.en.vtt\n",
            "[download] Destination: transcript_temp.en.vtt\n",
            "[download] 100% of   90.44KiB in 00:00:00 at 292.63KiB/s\n",
            "Processing subtitle file...\n"
          ]
        }
      ],
      "source": [
        "## Extracting Subtitle\n",
        "\n",
        "VIDEO_URL = yt_url\n",
        "VTT_FILENAME = \"transcript_temp.en.vtt\"\n",
        "\n",
        "def download_subtitles(video_url: str):\n",
        "    options = {\n",
        "        'skip_download': True,\n",
        "        'writesubtitles': False,\n",
        "        'writeautomaticsub': True,\n",
        "        'subtitleslangs': ['en'],\n",
        "        'subtitlesformat': 'vtt',\n",
        "        'outtmpl': 'transcript_temp.%(ext)s',\n",
        "        'noplaylist': True\n",
        "    }\n",
        "    with yt_dlp.YoutubeDL(options) as ydl:\n",
        "        ydl.download([video_url])\n",
        "\n",
        "\n",
        "def parse_and_clean_vtt(vtt_filename: str):\n",
        "    transcript_list = []\n",
        "    transcript_text = \"\"\n",
        "\n",
        "    for caption in webvtt.read(vtt_filename):\n",
        "        line = caption.text.strip()\n",
        "        transcript_list.append({\n",
        "            \"start\": caption.start,\n",
        "            \"end\": caption.end,\n",
        "            \"text\": line\n",
        "        })\n",
        "        transcript_text += line + \"\\n\"\n",
        "\n",
        "    lines = transcript_text.strip().splitlines()\n",
        "\n",
        "    # Remove consecutive duplicates\n",
        "    cleaned_lines = []\n",
        "    previous_line = None\n",
        "    for line in lines:\n",
        "        if line != previous_line:\n",
        "            cleaned_lines.append(line)\n",
        "        previous_line = line\n",
        "\n",
        "    cleaned_text = \"\\n\".join(cleaned_lines)\n",
        "    return cleaned_text, transcript_list\n",
        "\n",
        "\n",
        "print(\"Downloading subtitles...\")\n",
        "download_subtitles(VIDEO_URL)\n",
        "\n",
        "print(\"Processing subtitle file...\")\n",
        "cleaned_text, transcript_data = parse_and_clean_vtt(VTT_FILENAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWSK4-VQH8CG",
        "outputId": "01528517-4192-4620-f234-97055e4b6655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Remember how back in the day people\\n'\n",
            " 'would Google themselves? You type your\\n'\n",
            " 'name into a search engine')\n",
            "[{'end': '00:00:02.879',\n",
            "  'start': '00:00:02.869',\n",
            "  'text': 'Remember how back in the day people'},\n",
            " {'end': '00:00:04.950',\n",
            "  'start': '00:00:02.879',\n",
            "  'text': 'Remember how back in the day people\\n'\n",
            "          'would Google themselves? You type your'}]\n"
          ]
        }
      ],
      "source": [
        "cleaned_text, transcript_data = parse_and_clean_vtt(VTT_FILENAME)\n",
        "pprint(cleaned_text[:100])\n",
        "pprint(transcript_data[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "BASE_DIR = \"youtube_data\"\n",
        "TRANSCRIPT_DIR = os.path.join(BASE_DIR, \"transcript\")\n",
        "os.makedirs(TRANSCRIPT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def save_transcript_to_txt(comments, filename=\"transcript.txt\"):\n",
        "    filepath = os.path.join(TRANSCRIPT_DIR, filename)\n",
        "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, c in enumerate(comments, start=1):\n",
        "            f.write(f\"{i}. {c['text']}\\n\")\n",
        "\n",
        "\n",
        "def save_transcript_to_json(metadata, filename=\"transcript.json\"):\n",
        "    filepath = os.path.join(TRANSCRIPT_DIR, filename)\n",
        "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metadata, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "\n",
        "def save_transcript(cleaned_text, transcript_data, txt_filename=\"transcript_subtitle.txt\", json_filename=\"transcript_subtitle.json\"):\n",
        "    txt_path = os.path.join(TRANSCRIPT_DIR, txt_filename)\n",
        "    json_path = os.path.join(TRANSCRIPT_DIR, json_filename)\n",
        "\n",
        "    with open(txt_path, \"w\", encoding=\"utf-8\") as txt_out:\n",
        "        txt_out.write(cleaned_text)\n",
        "\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as json_out:\n",
        "        json.dump(transcript_data, json_out, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Saving all formats\n",
        "# save_transcript_to_txt(transcript_list)\n",
        "# save_transcript_to_json(transcript_list)\n",
        "save_transcript(cleaned_text, transcript_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKkcYsaOCrRX"
      },
      "source": [
        "### Text Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "24i-ZSVXFbnC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(metadata={}, page_content=\"Remember how back in the day people\\nwould Google themselves? You type your\\nname into a search engine and you see\\nwhat it knows about you? Well, the\\nmodern equivalent of that is to do the\\nsame thing with a chatbot. So, when I\\nask a large language model, who is\\nMartin Keen? Well, the response varies\\ngreatly depending upon which model I'm\\nasking because different models, they\\nhave different training data sets. They\\nhave different knowledge cutoff dates.\\nSo what a given model knows about me?\\nWell, it differs greatly. But how could\\nwe improve the model's answer? Well,\\nthere's three ways. So let's start with\\na model here. And we're going to see how\\nwe can improve its responses. Well, the\\nfirst thing it could do is it could go\\nout and it could perform a search. a\\nsearch for new data that either wasn't\\nin its training data set or it was just\\ndata that became available after the\\nmodel finished training. And then it\\ncould incorporate those results from the\\nsearch back into its answer. That is\"),\n",
            " Document(metadata={}, page_content=\"in its training data set or it was just\\ndata that became available after the\\nmodel finished training. And then it\\ncould incorporate those results from the\\nsearch back into its answer. That is\\ncalled rag or retrieval augmented\\ngeneration. That's one method. Or we\\ncould pick a specialized model, a model\\nthat's been trained on, let's say,\\ntranscripts of these videos. That would\\nbe an example of something called fine\\ntuning. Or we could ask the model a\\nquery that better specifies what we're\\nlooking for. So maybe the LLM already\\nknows plenty about the Martin Keen of\\nthe world, but let's tell the model that\\nwe're referring to the Martin Keen who\\nworks at IBM rather than the Martin Keen\\nthat founded Keen Shoes. That is an\\nexample of prompt\\nengineering. Three ways to get better\\noutputs out of large language models,\\neach with their pluses and\\nminuses. Let's start with rag. So, let's\\nbreak it down. First, there's retrieval.\\nSo, retrieval of external up-to-ate\"),\n",
            " Document(metadata={}, page_content=\"outputs out of large language models,\\neach with their pluses and\\nminuses. Let's start with rag. So, let's\\nbreak it down. First, there's retrieval.\\nSo, retrieval of external up-to-ate\\ninformation. Then, there's augmentation.\\nThat's augmentation of the original\\nprompt with the retrieved information\\nadded in. And then, finally, there's\\ngeneration. That's generation of a\\nresponse based on all of this enriched\\ncontext. So, we can think of it like\\nthis. So we start with a query and the\\nquery comes in to a large language\\nmodel. Now what rag is going to do is\\nit's first going to go searching through\\na corpus of information. So we have this\\ncorpus here full of some sort of data.\\nNow perhaps that's your organization's\\ndocuments. So it might be spreadsheets,\\nPDFs, internal wiks, you know, stuff\\nlike that. But unlike a typical search\\nengine that just matches keywords, Rag\\nconverts both your question, the query,\\nand all of the documents into something\\ncalled\\nvector\"),\n",
            " Document(metadata={}, page_content=\"like that. But unlike a typical search\\nengine that just matches keywords, Rag\\nconverts both your question, the query,\\nand all of the documents into something\\ncalled\\nvector\\nembeddings. So these are all converted\\ninto vectors. essentially turning words\\nand phrases into long lists of numbers\\nthat capture their meaning. So when you\\nask a query like what was our company's\\nrevenue growth last quarter? Well, Rag\\nwill find documents that are\\nmathematically similar in meaning to\\nyour question, even if they don't use\\nthe exact same words. So it might find\\ndocuments mentioning fourth quarter\\nperformance or quarterly sales. Those\\ndon't contain the keyword revenue\\ngrowth, but they are semantically\\nsimilar. Now, once rag finds the\\nrelevant information, it adds this\\ninformation back into your original\\nquery before passing it to the language\\nmodel. So instead of the model just kind\\nof guessing based on its training data,\\nit can now generate a response that\\nincorporates your actual facts and\")]\n",
            "Chunk Size: 13\n"
          ]
        }
      ],
      "source": [
        "transcript = cleaned_text\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = splitter.create_documents([transcript])\n",
        "pprint(chunks[:4])\n",
        "print(\"Chunk Size:\", len(chunks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xYFK7WXC2Ka"
      },
      "source": [
        "### Embedding Generation & Utilizing Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "jYXeS5T7FrC4"
      },
      "outputs": [],
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vector_store = FAISS.from_documents(chunks, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWYkp-NmFSVF",
        "outputId": "36f75b4d-b798-4e06-aeea-5c56c91befe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a71330c9-af01-4835-a465-cb6e91597d2c\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "print(vector_store.index_to_docstore_id[2])\n",
        "print(vector_store.get_by_ids(['d905d677-7a93-4c13-adf6-bd4215f31491']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zez1650EDN9J"
      },
      "source": [
        "### Retrieval Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "KEuoGUYOF3oG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000196C8095F30>, search_kwargs={'k': 4})\n"
          ]
        }
      ],
      "source": [
        "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
        "pprint(retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wvrsq08TGGNk",
        "outputId": "55ed9475-4497-4e53-d380-5c4c10bf68cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('in its training data set or it was just\\n'\n",
            " 'data that became available after the\\n'\n",
            " 'model finished training. And then it\\n'\n",
            " 'could incorporate those results from the\\n'\n",
            " 'search back into its answer. That is\\n'\n",
            " 'called rag or retrieval augmented\\n'\n",
            " \"generation. That's one method. Or we\\n\"\n",
            " 'could pick a specialized model, a model\\n'\n",
            " \"that's been trained on, let's say,\\n\"\n",
            " 'transcripts of these videos. That would\\n'\n",
            " 'be an example of something called fine\\n'\n",
            " 'tuning. Or we could ask the model a\\n'\n",
            " \"query that better specifies what we're\\n\"\n",
            " 'looking for. So maybe the LLM already\\n'\n",
            " 'knows plenty about the Martin Keen of\\n'\n",
            " \"the world, but let's tell the model that\\n\"\n",
            " \"we're referring to the Martin Keen who\\n\"\n",
            " 'works at IBM rather than the Martin Keen\\n'\n",
            " 'that founded Keen Shoes. That is an\\n'\n",
            " 'example of prompt\\n'\n",
            " 'engineering. Three ways to get better\\n'\n",
            " 'outputs out of large language models,\\n'\n",
            " 'each with their pluses and\\n'\n",
            " \"minuses. Let's start with rag. So, let's\\n\"\n",
            " \"break it down. First, there's retrieval.\\n\"\n",
            " 'So, retrieval of external up-to-ate')\n"
          ]
        }
      ],
      "source": [
        "retriever_docs = retriever.invoke('What is in the video')\n",
        "pprint(retriever_docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8y0wRmoDSVZ"
      },
      "source": [
        "### Augmentation Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "2-NeLx9wFHzw"
      },
      "outputs": [],
      "source": [
        "template = \"\"\" You are a helpful assistant. Answer ONLY from the provided transcript context. \n",
        "                 If the context is insufficient, just say you don't know.\n",
        "                 Context: {context}\n",
        "             \"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages([MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "                                           (\"system\", template), \n",
        "                                           (\"human\", \"question:{input}\")\n",
        "                                           ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "WI9BOZQwGizf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(id='77b88199-0940-4a68-89fc-61bbf04c6d1c', metadata={}, page_content=\"we've got documents that need to be\\nvector embeddings and we need to store\\nthese vector embeddings in a database.\\nAll of this adds to processing costs. It\\nadds to infrastructure costs to make\\nthis solution work. All right, next up,\\nfine tuning. So remember how we\\ndiscussed getting better answers about\\nme by training a model specifically on\\nlet's say my video transcripts? Well,\\nthat is fine tuning in action. So what\\nwe do with fine-tuning is we take a\\nmodel but specifically an existing model\\nand that existing model has broad\\nknowledge and then we're going to give\\nit additional specialized training on a\\nfocused data set. So this is now\\nspecialized to what we want to develop\\nparticular expertise on. Now during\\nfine-tuning, we're updating the model's\\ninternal parameters through additional\\ntraining. So the model starts out with\\nsome weights here like\\nthis. And those weights were optimized\\nduring its initial pre-training. And as\\nwe fine-tune, we're making small\"),\n",
            " Document(id='5b9011a2-2fd9-4de2-b15f-9b5a34dcbf72', metadata={}, page_content=\"training. So the model starts out with\\nsome weights here like\\nthis. And those weights were optimized\\nduring its initial pre-training. And as\\nwe fine-tune, we're making small\\nadjustments here to the model's weights\\nusing this specialized data set. So this\\nis being\\nincorporated. Now this process typically\\nuses supervised learning where we\\nprovide input output pairs that\\ndemonstrate the kind of responses we\\nwant. So for example, if we're\\nfine-tuning for technical support, we\\nmight provide thousands of examples of\\ncustomer queries and those would be\\npaired with correct technical responses.\\nThe model adjusts its weight through\\nback propagation to minimize the\\ndifference between its predicted outputs\\nand the targeted responses. So we're not\\njust teaching the model new facts here.\\nWe're actually modifying how it\\nprocesses information. The model is\\nlearning to recognize domain specific\\npatterns. So fine-tuning shows its\\nstrength when you particularly need a\\nmodel that has very deep\\ndomain\"),\n",
            " Document(id='6bb94c45-a883-4386-bd8f-0857dacd33bb', metadata={}, page_content=\"processes information. The model is\\nlearning to recognize domain specific\\npatterns. So fine-tuning shows its\\nstrength when you particularly need a\\nmodel that has very deep\\ndomain\\nexpertise. That's what we can really add\\nin with fine-tuning and also it's much\\nfaster specifically at inference time.\\nSo when we are putting the queries in,\\nit's faster than rag because it doesn't\\nneed to search through external data.\\nAnd because the knowledge is kind of\\nbaked into the model's weights, you\\ndon't need to maintain a separate vector\\ndatabase. But there's some downsides as\\nwell. Well, there's certainly issues\\nhere with the training complexity of all\\nof this. You're going to need thousands\\nof high quality training examples. There\\nare\\nalso issues with computational cost. The\\ncomputational cost for training this\\nmodel can be substantial and is going to\\nrequire a whole bunch of GPUs. And\\nthere's also challenges related to\\nmaintenance as well because unlike rag\\nwhere you can easily add new documents\"),\n",
            " Document(id='f9a1dd06-8d9f-4377-ba43-c67b6af9ba85', metadata={}, page_content=\"receives this prompt and it processes it\\nthrough a series of\\nlayers. And these layers are essentially\\nattention mechanisms. And each one\\nfocuses on different aspects of your\\nprompt text that came in. And by\\nincluding specific elements in your\\nprompt, so examples or context or how\\nyou want the format to look, you're\\ndirecting the model's attention to\\nrelevant patterns it learned during\\ntraining. So for example, telling a\\nmodel to think about this step by step\\nthat activates patterns it learned from\\ntraining data where methodical reasoning\\nled to accurate results. So a\\nwellengineered prompt can transform a\\nmodel's output without any additional\\ntraining or without data retrieval. So\\ntake an example of a of a prompt. Let's\\nsay we say is this code secure. It's not\\na very good prompt. An engineer prompt.\\nIt might read a bit more like this. It's\\nmuch more detailed. Now we haven't\\nchanged the model. We haven't added new\\ndata. we've just better activated its\")]\n"
          ]
        }
      ],
      "source": [
        "question = \"What is this topic about?\"\n",
        "retrieved_docs = retriever.invoke(question)\n",
        "pprint(retrieved_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "bKwpvAo5G_Pk",
        "outputId": "26f0efd0-b35f-44dd-9735-dfc41c1ab86d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(\"we've got documents that need to be\\n\"\n",
            " 'vector embeddings and we need to store\\n'\n",
            " 'these vector embeddings in a database.\\n'\n",
            " 'All of this adds to processing costs. It\\n'\n",
            " 'adds to infrastructure costs to make\\n'\n",
            " 'this solution work. All right, next up,\\n'\n",
            " 'fine tuning. So remember how we\\n'\n",
            " 'discussed getting better answers about\\n'\n",
            " 'me by training a model specifically on\\n'\n",
            " \"let's say my video transcripts? Well,\\n\"\n",
            " 'that is fine tuning in action. So what\\n'\n",
            " 'we do with fine-tuning is we take a\\n'\n",
            " 'model but specifically an existing model\\n'\n",
            " 'and ')\n"
          ]
        }
      ],
      "source": [
        "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "pprint(context_text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "_bikWKZWDiqB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatPromptValue(messages=[SystemMessage(content=\" You are a helpful assistant. Answer ONLY from the provided transcript context. \\n                 If the context is insufficient, just say you don't know.\\n                 Context: we've got documents that need to be\\nvector embeddings and we need to store\\nthese vector embeddings in a database.\\nAll of this adds to processing costs. It\\nadds to infrastructure costs to make\\nthis solution work. All right, next up,\\nfine tuning. So remember how we\\ndiscussed getting better answers about\\nme by training a model specifically on\\nlet's say my video transcripts? Well,\\nthat is fine tuning in action. So what\\nwe do with fine-tuning is we take a\\nmodel but specifically an existing model\\nand that existing model has broad\\nknowledge and then we're going to give\\nit additional specialized training on a\\nfocused data set. So this is now\\nspecialized to what we want to develop\\nparticular expertise on. Now during\\nfine-tuning, we're updating the model's\\ninternal parameters through additional\\ntraining. So the model starts out with\\nsome weights here like\\nthis. And those weights were optimized\\nduring its initial pre-training. And as\\nwe fine-tune, we're making small\\n\\ntraining. So the model starts out with\\nsome weights here like\\nthis. And those weights were optimized\\nduring its initial pre-training. And as\\nwe fine-tune, we're making small\\nadjustments here to the model's weights\\nusing this specialized data set. So this\\nis being\\nincorporated. Now this process typically\\nuses supervised learning where we\\nprovide input output pairs that\\ndemonstrate the kind of responses we\\nwant. So for example, if we're\\nfine-tuning for technical support, we\\nmight provide thousands of examples of\\ncustomer queries and those would be\\npaired with correct technical responses.\\nThe model adjusts its weight through\\nback propagation to minimize the\\ndifference between its predicted outputs\\nand the targeted responses. So we're not\\njust teaching the model new facts here.\\nWe're actually modifying how it\\nprocesses information. The model is\\nlearning to recognize domain specific\\npatterns. So fine-tuning shows its\\nstrength when you particularly need a\\nmodel that has very deep\\ndomain\\n\\nprocesses information. The model is\\nlearning to recognize domain specific\\npatterns. So fine-tuning shows its\\nstrength when you particularly need a\\nmodel that has very deep\\ndomain\\nexpertise. That's what we can really add\\nin with fine-tuning and also it's much\\nfaster specifically at inference time.\\nSo when we are putting the queries in,\\nit's faster than rag because it doesn't\\nneed to search through external data.\\nAnd because the knowledge is kind of\\nbaked into the model's weights, you\\ndon't need to maintain a separate vector\\ndatabase. But there's some downsides as\\nwell. Well, there's certainly issues\\nhere with the training complexity of all\\nof this. You're going to need thousands\\nof high quality training examples. There\\nare\\nalso issues with computational cost. The\\ncomputational cost for training this\\nmodel can be substantial and is going to\\nrequire a whole bunch of GPUs. And\\nthere's also challenges related to\\nmaintenance as well because unlike rag\\nwhere you can easily add new documents\\n\\nreceives this prompt and it processes it\\nthrough a series of\\nlayers. And these layers are essentially\\nattention mechanisms. And each one\\nfocuses on different aspects of your\\nprompt text that came in. And by\\nincluding specific elements in your\\nprompt, so examples or context or how\\nyou want the format to look, you're\\ndirecting the model's attention to\\nrelevant patterns it learned during\\ntraining. So for example, telling a\\nmodel to think about this step by step\\nthat activates patterns it learned from\\ntraining data where methodical reasoning\\nled to accurate results. So a\\nwellengineered prompt can transform a\\nmodel's output without any additional\\ntraining or without data retrieval. So\\ntake an example of a of a prompt. Let's\\nsay we say is this code secure. It's not\\na very good prompt. An engineer prompt.\\nIt might read a bit more like this. It's\\nmuch more detailed. Now we haven't\\nchanged the model. We haven't added new\\ndata. we've just better activated its\\n             \", additional_kwargs={}, response_metadata={}), HumanMessage(content='question:What is this topic about?', additional_kwargs={}, response_metadata={})])\n"
          ]
        }
      ],
      "source": [
        "chat_history = []\n",
        "final_prompt = prompt.invoke({\"context\": context_text, \"input\": question, \"chat_history\":chat_history})\n",
        "pprint(final_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxxcV2C_DXqt"
      },
      "source": [
        "### Generation Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX6vxSoUHBok",
        "outputId": "eade1e56-b8af-4b7e-c34b-08a4a1b0da00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('The topic is about fine-tuning, which involves taking an existing model with '\n",
            " 'broad knowledge and giving it additional specialized training on a focused '\n",
            " 'data set to develop particular expertise. It also discusses prompt '\n",
            " 'engineering, which involves including specific elements in a prompt to '\n",
            " \"direct the model's attention to relevant patterns learned during training.\")\n"
          ]
        }
      ],
      "source": [
        "answer = llm.invoke(final_prompt)\n",
        "pprint(answer.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH2Ph0NcDlo5"
      },
      "source": [
        "### Building End-to-End Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "VGezE1qYQJ76"
      },
      "outputs": [],
      "source": [
        "def format_docs(retrieved_docs):\n",
        "  context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "  return context_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever_chain = RunnableMap({\n",
        "    \"context\": lambda inputs: retriever.invoke(inputs[\"input\"]),\n",
        "    \"input\": lambda inputs: inputs[\"input\"],\n",
        "    \"chat_history\": lambda inputs: inputs[\"chat_history\"],\n",
        "})\n",
        "\n",
        "format_chain = RunnableLambda(lambda x: {**x, \"context\": format_docs(x[\"context\"])})\n",
        "\n",
        "rag_chain = retriever_chain | format_chain | prompt | llm "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The video discusses three ways to improve the outputs of large language models (LLMs): Retrieval Augmented Generation (RAG), fine-tuning, and prompt engineering. RAG involves searching for new data and incorporating it into the answer. Fine-tuning involves training a model on a specialized data set. Prompt engineering involves better specifying what we're looking for. The video also touches on the pluses and minuses of each approach.\n"
          ]
        }
      ],
      "source": [
        "chat_history = []\n",
        "\n",
        "question = \"Can you suumarise the Video\"\n",
        "\n",
        "response = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
        "\n",
        "chat_history.extend([HumanMessage(content=question), AIMessage(content=response.content)])\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Okay, let's elaborate on each of the three methods for improving LLM outputs: Retrieval Augmented Generation (RAG), Fine-tuning, and Prompt Engineering, along with their pros and cons.\n",
            "\n",
            "**1. Retrieval Augmented Generation (RAG)**\n",
            "\n",
            "*   **What it is:** RAG is a technique that enhances LLMs by allowing them to access and incorporate information from external knowledge sources *before* generating a response. Think of it as giving the LLM access to a \"library\" or a database to research before answering your question.\n",
            "\n",
            "*   **How it works:**\n",
            "\n",
            "    1.  **User Query:** The user submits a question or prompt.\n",
            "    2.  **Retrieval:** The system identifies relevant documents or information from an external knowledge base (e.g., a database, a collection of documents, a website). This is often done using semantic search, which looks for meaning rather than just keywords.\n",
            "    3.  **Augmentation:** The retrieved information is combined with the original user query.\n",
            "    4.  **Generation:** The augmented prompt (original query + retrieved information) is fed to the LLM. The LLM then generates a response based on *both* the original query and the retrieved context.\n",
            "\n",
            "*   **Pros:**\n",
            "\n",
            "    *   **Improved Accuracy:** By grounding responses in external knowledge, RAG reduces hallucinations (making things up) and improves factual accuracy.\n",
            "    *   **Access to Up-to-Date Information:** LLMs are often trained on static datasets, meaning they don't inherently know about recent events. RAG allows them to access and incorporate current information.\n",
            "    *   **Transparency and Explainability:**  You can often trace the LLM's answer back to the specific source documents used, making the reasoning more transparent.\n",
            "    *   **Reduced Training Costs:**  RAG avoids the need to retrain the entire LLM every time new information becomes available. You only need to update the external knowledge base.\n",
            "    *   **Domain Specificity:** RAG can be used to provide the LLM with domain specific knowledge, allowing it to answer questions about niche topics.\n",
            "\n",
            "*   **Cons:**\n",
            "\n",
            "    *   **Complexity:** Implementing RAG can be more complex than simply using an LLM directly. It requires setting up and maintaining a knowledge base and a retrieval system.\n",
            "    *   **Retrieval Quality:** The quality of the retrieved information is\n"
          ]
        }
      ],
      "source": [
        "question = \"Please Elaborate\"\n",
        "\n",
        "response = rag_chain.invoke({\n",
        "    \"input\": question,\n",
        "    \"chat_history\": chat_history\n",
        "})\n",
        "\n",
        "chat_history.extend([\n",
        "    HumanMessage(content=question),\n",
        "    AIMessage(content=response.content)\n",
        "])\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Can you suumarise the Video', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"The video discusses three ways to improve the outputs of large language models (LLMs): Retrieval Augmented Generation (RAG), fine-tuning, and prompt engineering. RAG involves searching for new data and incorporating it into the answer. Fine-tuning involves training a model on a specialized data set. Prompt engineering involves better specifying what we're looking for. The video also touches on the pluses and minuses of each approach.\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Please Elaborate', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Okay, let\\'s elaborate on each of the three methods for improving LLM outputs: Retrieval Augmented Generation (RAG), Fine-tuning, and Prompt Engineering, along with their pros and cons.\\n\\n**1. Retrieval Augmented Generation (RAG)**\\n\\n*   **What it is:** RAG is a technique that enhances LLMs by allowing them to access and incorporate information from external knowledge sources *before* generating a response. Think of it as giving the LLM access to a \"library\" or a database to research before answering your question.\\n\\n*   **How it works:**\\n\\n    1.  **User Query:** The user submits a question or prompt.\\n    2.  **Retrieval:** The system identifies relevant documents or information from an external knowledge base (e.g., a database, a collection of documents, a website). This is often done using semantic search, which looks for meaning rather than just keywords.\\n    3.  **Augmentation:** The retrieved information is combined with the original user query.\\n    4.  **Generation:** The augmented prompt (original query + retrieved information) is fed to the LLM. The LLM then generates a response based on *both* the original query and the retrieved context.\\n\\n*   **Pros:**\\n\\n    *   **Improved Accuracy:** By grounding responses in external knowledge, RAG reduces hallucinations (making things up) and improves factual accuracy.\\n    *   **Access to Up-to-Date Information:** LLMs are often trained on static datasets, meaning they don\\'t inherently know about recent events. RAG allows them to access and incorporate current information.\\n    *   **Transparency and Explainability:**  You can often trace the LLM\\'s answer back to the specific source documents used, making the reasoning more transparent.\\n    *   **Reduced Training Costs:**  RAG avoids the need to retrain the entire LLM every time new information becomes available. You only need to update the external knowledge base.\\n    *   **Domain Specificity:** RAG can be used to provide the LLM with domain specific knowledge, allowing it to answer questions about niche topics.\\n\\n*   **Cons:**\\n\\n    *   **Complexity:** Implementing RAG can be more complex than simply using an LLM directly. It requires setting up and maintaining a knowledge base and a retrieval system.\\n    *   **Retrieval Quality:** The quality of the retrieved information is', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('content', 'Okay, let\\'s elaborate on each of the three methods for improving LLM outputs: Retrieval Augmented Generation (RAG), Fine-tuning, and Prompt Engineering, along with their pros and cons.\\n\\n**1. Retrieval Augmented Generation (RAG)**\\n\\n*   **What it is:** RAG is a technique that enhances LLMs by allowing them to access and incorporate information from external knowledge sources *before* generating a response. Think of it as giving the LLM access to a \"library\" or a database to research before answering your question.\\n\\n*   **How it works:**\\n\\n    1.  **User Query:** The user submits a question or prompt.\\n    2.  **Retrieval:** The system identifies relevant documents or information from an external knowledge base (e.g., a database, a collection of documents, a website). This is often done using semantic search, which looks for meaning rather than just keywords.\\n    3.  **Augmentation:** The retrieved information is combined with the original user query.\\n    4.  **Generation:** The augmented prompt (original query + retrieved information) is fed to the LLM. The LLM then generates a response based on *both* the original query and the retrieved context.\\n\\n*   **Pros:**\\n\\n    *   **Improved Accuracy:** By grounding responses in external knowledge, RAG reduces hallucinations (making things up) and improves factual accuracy.\\n    *   **Access to Up-to-Date Information:** LLMs are often trained on static datasets, meaning they don\\'t inherently know about recent events. RAG allows them to access and incorporate current information.\\n    *   **Transparency and Explainability:**  You can often trace the LLM\\'s answer back to the specific source documents used, making the reasoning more transparent.\\n    *   **Reduced Training Costs:**  RAG avoids the need to retrain the entire LLM every time new information becomes available. You only need to update the external knowledge base.\\n    *   **Domain Specificity:** RAG can be used to provide the LLM with domain specific knowledge, allowing it to answer questions about niche topics.\\n\\n*   **Cons:**\\n\\n    *   **Complexity:** Implementing RAG can be more complex than simply using an LLM directly. It requires setting up and maintaining a knowledge base and a retrieval system.\\n    *   **Retrieval Quality:** The quality of the retrieved information is')\n",
            "('additional_kwargs', {})\n",
            "('response_metadata', {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'MAX_TOKENS', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []})\n",
            "('type', 'ai')\n",
            "('name', None)\n",
            "('id', 'run--38e2c146-66a8-48b2-8d2a-2b491f51320e-0')\n",
            "('example', False)\n",
            "('tool_calls', [])\n",
            "('invalid_tool_calls', [])\n",
            "('usage_metadata', {'input_tokens': 96, 'output_tokens': 490, 'total_tokens': 586, 'input_token_details': {'cache_read': 0}})\n"
          ]
        }
      ],
      "source": [
        "for details in response:\n",
        "    print(details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "fmYnYqbWQWLi"
      },
      "outputs": [],
      "source": [
        "## Chain without History\n",
        "\n",
        "#  parallel_chain = RunnableParallel({'context': retriever | RunnableLambda(format_docs),\n",
        "#                                    'question': RunnablePassthrough()\n",
        "#                                    })\n",
        "# retriever_chain = parallel_chain.invoke('who is Demis')\n",
        "# retriever_chain\n",
        "\n",
        "# parser = StrOutputParser()\n",
        "# rag_chain = parallel_chain | prompt | llm | parser\n",
        "# response = rag_chain.invoke('Can you summarize the video')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
